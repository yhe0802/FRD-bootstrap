\documentclass[12pt,]{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{pgfplots}
\usepackage{geometry}
\usepackage{threeparttable}
\geometry{margin=1in}


% Define theorem styles
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{algorithm}{Algorithm}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newcommand{\textif}{\text{if}}
\newcommand{\otherwise}{\text{otherwise}}

\DeclareMathOperator{\1}{\mathbbm{1}}
\DeclareMathOperator{\E}{\mathbbm{E}}
\DeclareMathOperator{\V}{\mathbbm{V}}
\DeclareMathOperator{\Cov}{\mathbbm{C}ov}
\DeclareMathOperator{\R}{\mathbbm{R}}
\DeclareMathOperator{\tr}{tr}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\plim}{plim}

% Local sum, average, and scaled average macros.
% - 's' stands for 'set index'
% - 'o' stands for 'ordered index'
\newcommand\ssum[3][\pm]{\sum_{#2 \in I_{#1}({#3})}}
\newcommand\savg[3][\pm]{\tfrac{1}{M_{#1}({#3})} \ssum[#1]{#2}{#3}}
\newcommand\sclt[3][\pm]{\tfrac{1}{\sqrt{M_{#1}({#3})}} \ssum[#1]{#2}{#3}}

\newcommand\osum[3][\pm]{\sum_{#2 = 1}^{M_{#1}({#3})}}
\newcommand\oavg[3][\pm]{\tfrac{1}{M_{#1}({#1})} \osum[#1]{#2}{#3}}
\newcommand\oclt[3][\pm]{\tfrac{1}{\sqrt{M_{#1}({#1})}} \osum[#1]{#2}{#3}}

\newcommand{\B}{\boldsymbol{B}}
\newcommand{\Bf}{\mathfrak{B}}
\renewcommand{\Pr}{\operatorname{Pr}}
\newcommand{\epb}{\boldsymbol{\varepsilon}}
\newcommand{\Chi}{\boldsymbol{\chi}}

\newcommand{\Bft}{\tilde{\Bf}}
\newcommand{\Ct}{\tilde{C}}
\newcommand{\Gammat}{\tilde{\Gamma}}
\newcommand{\Psit}{\tilde{\Psi}}
\newcommand{\Vt}{\mathcal{V}}



\begin{document}
\title{Using Wild Bootstrap to Construct Robust Confidence Intervals for Fuzzy Regression Discontinuity Designs}
\author{Yang He\thanks{Department of Economics, Iowa State University. Email: yanghe@iastate.edu.}}
\date{December 2016}
\maketitle

\begin{abstract}\noindent
A new wild bootstrap procedure is proposed to correct bias and construct valid confidence intervals in fuzzy regression discontinuity designs. This procedure uses a wild bootstrap based on second order local polynomials to estimate and remove the bias from linear models. The bias-corrected estimator is then bootstrapped itself to generate valid confidence intervals. While the conventional confidence intervals generated by adopting MSE-optimal bandwidth is asymptotically not valid, the confidence intervals generated by this procedure have correct coverage under conditions similar to Calonico, Cattaneo and Titiunik's(2014, \textit{Econometrica}) analytical correction. Simulation studies provide evidence that this new method is as accurate as the analytical corrections when applied to a variety of data generating processes featuring heteroskedasticity, endogeneity and clustering. As an example, its usage is demonstrated through a reanalysis of the scholastic achievement data used by \cite{angrist1999using}.

{\bf Keywords:} regression discontinuity, nonparametric, wild bootstrap, average treatment effect.
\end{abstract}

\section{Introduction}
The idea of regression discontinuity (RD) design was firstly used by \cite{thistlethwaite1960regression} to estimate the causal effect of merit awards on future academic outcomes. In their application, the discontinuity in receiving merit awards as a function of test scores (refereed to in literature as ``forcing variable" or ``running variable", which determines the treatment assignment) creates a local randomized experiment, which allows researchers to identify the causal effect at the point of discontinuity. The idea of RD designs did not get much attention from economists in its early years, but the past decade has seen its increasing popularity in analyzing the causal impact of policies and interventions in social science. \cite{imbens2008regression} and \cite{lee2010regression} provide recent reviews of this literature with many examples.

The identification in RD designs relies on the assumption that units arbitrarily close to the cutoff are credibly similar in predetermined characteristics. Under this ``smoothness" condition, one can essentially compare units slightly above the cutoff and units slightly below the cutoff, and the difference in outcomes (probably after rescaling) can be thought of as being induced by exogenous changes in treatment, giving it an interpretation of treatment effect. When the running variable completely determines the treatment, the probability of being treated jumps from zero to one at the cutoff (sharp RD designs). On the contrary, when the running variable does not entirely determine the treatment, there are both treated and untreated units on each side of the cutoff. This treatment misassignment was studied in a series of work by \cite{trochim1980relative} and \cite{trochim1984research} and was called ``fuzzy" RD design thereafter. Directly comparing the outcomes on both sides of the cutoff results in an ``intent-to-treat" effect but not the actual treatment effect because this difference is contributed only by part of the units. As in a Wald formulation of the treatment effect in instrumental variable setting, the true treatment effect can be recovered by taking the ratio of difference in outcomes and difference in treatment probabilities at the cutoff. Even when units are self-selected to treatment based on anticipated gains, \cite{hahn2001identification} show that this ratio can be interpreted as the local average treatment effect (LATE) under proper assumptions.

The identification of RD designs occurs exactly at the cutoff, which unavoidably requires extrapolation. Established by \cite{fan1992design} and advocated by \cite{hahn2001identification}, the desirable boundary property of local linear models makes them almost standard practice in estimating RD designs. An important tunning variable in these nonparametric models is the bandwidth $h$, which controls the trade-off between bias and variance. One very popular choice of this tunning variable under the setting of RD designs is the bandwidth selector proposed by \cite{imbens2011optimal}, which minimizes the asymptotic mean squared error (AMSE) of the treatment effect estimator. This bandwidth selector has the form $h = O_p(n^{-1/5})$, where $n$ is the number of observations. However, as is shown by \cite{hahn2001identification}, a bandwidth choice of $h = O_p(n^{-1/5})$ leads to an asymptotic normal distribution of the treatment effect estimator centered at the true treatment effect plus a non-negligible bias term. Ignoring this bias term invalidates confidence intervals based on Wald test. Simulation studies on sharp RD designs in \cite{calonico2014robust}, henceforth ``CCT," also confirm that conventional confidence intervals have empirical coverage well below their nominal levels. As a result, it is common practice to use ad-hoc bandwidths which shrink at a rate more than $n^{-1/5}$ so that the bias term vanishes faster in a hope that the asymptotic normality is not distorted.

CCT solve this problem by firstly re-centering the conventional point estimator with estimated bias term and then rescaling it by a unconventional standard error which takes into consideration the additional variability of the estimated bias. This approach results in a bias-corrected point estimator which is asymptotically normal with weaker assumptions on the bandwidth choice. Confidence intervals based on this method are accurate even when the AMSE optimal bandwidths are used.

In this paper, a wild bootstrap procedure is proposed as an alternative to CCT's robust inference method for fuzzy RD designs. It is theoretically proved that the new bootstrap procedure is asymptotically equivalent to CCT's and supported by simulations that it performs well with finite sample. Compared with CCT's analytical method, the bootstrap procedure is very straightforward and does not require intensive analytical derivations. In addition, since the bootstrap is motivated by mimicking the true data generating process, it has the flexibility to accommodate dependent data by adjusting the resampling algorithm accordingly. In particular, this paper demonstrates how the proposed bootstrap procedure can be applied to clustered data and perform at least as good as the analytical robust method.

The wild bootstrap procedure exploits CCT's theoretical insight by resampling from higher order local polynomials. In particular, the local linear models are estimated as usual for both outcome and treatment, resulting in a conventional biased estimator. To estimate the bias, additional local quadratic models are estimated. These second order polynomials together with the potentially correlated residuals represent the true data generating process (DGP) for bootstrap. The bias of the conventional estimator from local linear models is therefore known under this bootstrap DGP and can be calculated by averaging the error of the linear model's estimates across many bootstrap replications. Though the local quadratic models are also not bias free, it can be shown that its bias converges to zero at a faster rate, fast enough that the bias of the local linear model can be estimated and removed using the second order polynomial. This approach is described in detail by Algorithm \ref{al: bias estimation} and the resulting bias-corrected estimator is shown to be asymptotically normal with mean zero in  Theorem \ref{th: bootstrap bias correction}.

This bias correction procedure introduces additional variability because the bias is calculated by assuming that local quadratic models represent the true DGP. However, these local quadratic models also come with uncertainty because of sampling error. So an iterated bootstrap procedure \citep{hall1988bootstrap} is adopted to accommodate this additional variability: generate many bootstrap datasets from local quadratic models and calculate bias-corrected estimate for each of them. The resulting empirical distribution of bias-corrected estimate is then used to construct confidence intervals. This procedure is in line with CCT's approach, where the variance of estimated bias term and the covariance between estimated bias and original point estimator are derived analytically. This complex adjustment to the original variance is automatically embedded in the iterated bootstrap. The detailed implementation steps are described in Algorithm \ref{al: distribution}, and the resulting confidence intervals are shown to be asymptotically valid in Theorem \ref{th: bootstrap confidence intervals}.

This paper is closely related to the work by \cite{bartalotti2016bootstrap}, who look at the robust inference in sharp RD designs. Given that this is only a special case in RD designs, the current paper provides important generalization in several dimensions. First, it borrows the idea of bootstrapping IV models and adapts that to a more general fuzzy RD design. Second, its validity is extended and theoretically proved to any order of local polynomials and any order of derivatives of interests. Lastly, its flexibility and capability to accommodate clustered data is discussed and confirmed by simulation studies. 

The paper is organized as follows. Section 2 describes the basic fuzzy RD approach, its usual implementation, and the CCT's robust inference method. Section 3 presents the proposed bootstrap procedures to estimate bias and construct confidence interval. Their asymptotic properties are discussed and summarized in two theorems. Section 4 provides simulation evidence that the bootstrap procedure effectively reduces bias and generates valid confidence intervals. An extended application to clustered data is discussed in Section 5. Section 6 demonstrates the usage of this bootstrap procedure by applying it to the scholastic achievement data used by \cite{angrist1999using}. Finally, Section 7 concludes.


\section{Background}
\label{se: background}

This section provides additional details of identification assumptions and traditional estimation methods in fuzzy RD designs. It also briefly introduces the robust confidence interval proposed by CCT. Notations defined in this section and following sections are consistent with CCT where possible to aid readers familiar with that paper.

In a typical fuzzy RD setting, researchers are interested in the local causal effect of treatment at a given cutoff. For any unit $i$, a triple $(X_i, T_i, Y_i)$ is observed, where $X_i$ is a continuous running variable which determines treatment assignment, $T_i$ is a binary variable which indicates actual treatment status and $Y_i$ is the outcome. In sharp RD designs, the treatment actually received is the same as the assigned treatment, i.e., $T_i = \1(X_i \ge c)$, with $c$ being the cutoff. In fuzzy RD designs, however, the received treatment is not a deterministic function of running variable $X_i$. Instead, the probability $\Pr (T_i = 1 \mid X_i)$ is between zero and one in both sides but experiences a sudden change at the cutoff. Without loss of generality, the cutoff $c$ can be reset to zero. If assigned to treatment ($X_i \ge 0$), unit $i$'s actual treatment status and outcome are represented by functions $T_i(1)$ and $Y_i(1)$, otherwise $T_i(0)$ and $Y_i(0)$. Thus the observed treatment status and outcome are
\begin{align*}
	T_i & = T_i(0) \1 (X_i < 0) + T_i(1) \1 (X_i \ge 0) \\
	Y_i & = Y_i(0) \1 (X_i < 0) + Y_i(1) \1 (X_i \ge 0).
\end{align*}
This is essentially an identification problem with missing data. For each unit $i$'s outcome, either $Y_i(0)$ or $Y_i(1)$ is observed. The data itself is uninformative in terms of treatment effect because counterfactual outcome could be totally arbitrary. However, under continuity and smoothness conditions on $T_i(0), Y_i(0), T_i(1)$ and $Y_i(1)$ around the cutoff $X_i = 0$, to identify the treatment effect for units just at the cutoff is possible and the estimand of interest is
\begin{equation}
	\label{eq: frd definition}
	\zeta = \frac{\tau_Y}{\tau_T} = \frac{\E (Y_i(1) \mid X_i = 0) - \E (Y_i(0) \mid X_i = 0)}{\E (T_i(1) \mid X_i = 0) - \E (T_i(0) \mid X_i = 0)},
\end{equation}
where the symbol $\E$ represents the expectation and $\tau_Y$ and $\tau_T$ represent the sharp RD estimators, i.e., difference in expectations at the cutoff. Intuitively, this is a Wald estimator in the limit where the assigned treatment serves as an instrument. The reduced-form difference in expected outcome, $\tau_Y$, reveals the ``intent-to-treat" (ITT) effect. The treatment effect is recovered by dividing ITT effect by the first stage difference in treatment probabilities. When the treatment effect is not constant across units, $\zeta$ should be interpreted with caution. If treatment status is independent of treatment effects at the cutoff, $\zeta$ is the average treatment effect (ATE) at the cutoff. This assumption rules out self-selection based on anticipated gain. \cite{hahn2001identification} show that under a less restrictive assumption that the running variable is independent of the joint distribution of treatment effect and treatment status at the cutoff, the local average treatment effect (LATE) is identified.

The formula for $\zeta$ shows that it is a ratio of two sharp RD estimators. Due to this symmetry, I use ``$Z$'' as a placeholder for either outcome variable $Y$ or treatment variable $T$ to ease the notation. In addition, I introduce conditional expectations $\mu_{Z+}(x)$ and $\mu_{Z-}(x)$, conditional variances $\sigma_{Z+}^2(x)$ and $\sigma_{Z-}^2(x)$, the $\eta$th order derivative of conditional expectations $\mu_{Z+}^{(\eta)}(x)$ and $\mu_{Z-}^{(\eta)}(x)$ and their limits. Formally, they are defined as
\begin{align*}
	\mu_{Z+}(x) &= \E( Z_{i}(1) \mid X_{i}=x ) 
	&\mu_{Z-}(x) &= \E( Z_{i}(0) \mid X_{i}=x ) \\
	\sigma^{2}_{Z+}(x) &= \V( Z_{i}(1) \mid X_{i}=x )
	&\sigma^{2}_{Z-}(x) &=\V( Z_{i}(0) \mid X_{i}=x ) \\
	\mu_{Z+}^{(\eta)}(x) &=\frac{d^{\eta}\mu_{Z+}(x)}{dx^{\eta}}
	&\mu_{Z-}^{(\eta)}(x) &=\frac{d^{\eta}\mu_{Z-}(x)}{dx^{\eta}} \\
	\mu^{(\eta)}_{Z+} &= \lim_{x \rightarrow 0^{+}}\mu_{Z+}^{(\eta)}(x)
	&\mu^{(\eta)}_{Z-} &= \lim_{x \rightarrow 0^{-}}\mu_{Z-}^{(\eta)}(x)
\end{align*}
where the symbol $\V(\cdot)$ represents variance.  The treatment effect $\zeta$ is nonparametrically estimable because $\mu_{Z-}$ and $\mu_{Z+}$ can be estimated consistently under Assumption \ref{as: DGP behavior}, which lists standard conditions in the fuzzy RD literature. (See, in particular, \citealp{hahn2001identification}, \citealp{porter2003estimation} and CCT.)

\begin{assumption}[Behavior of the DGP near the cutoff]
	\label{as: DGP behavior}
	The random variables $\{X_i, T_i, Y_i\}_{i=1}^n$ form a random sample of size $n$. There exists a positive number $\kappa_0$ such that the following
	conditions hold for all $x$ in the neighborhood $(-\kappa_{0},\ \kappa_{0})$ around zero:
	
	\begin{enumerate}
		\item The density of $X_i$ is continuous and bounded away from zero at $x$.
		\item $\E[Z_i^4 \mid X_i = x]$ is bounded.
		\item $\mu_{Z-}(x)$ and $\mu_{Z+}(x)$ are three times continuously differentiable.
		\item $\sigma_{Z-}^2(x)$ and $\sigma_{Z+}^2(x)$ are continuous and bounded away from zero.
		\item $\mu_{T-}(0) \ne \mu_{T+}(0)$.
	\end{enumerate}
\end{assumption}

Part 1 ensures that the number of data points arbitrarily close to the cutoff increases as the sample size grows. Part 3 imposes necessary smoothness condition to allow an approximation by second order polynomials. Part 2 and 4 put standard restrictions on moments to ensure that the estimated local polynomials are well behaved. Part 5 requires that the treatment assignment as an instrument is valid, in the sense that it induces a first stage difference in treatment probability. In practice, local polynomial regression is widely used to estimate RD designs because of nice boundary properties.\footnote{See \cite{fan1996local} for discussions on the boundary properties of local polynomial regression. See \cite{gelman2014high} for discussions on the choices of global and local polynomial regression and its order.} As an illustration, I focus here on local linear regression using kernel function $K(\cdot)$. For simplicity, suppose a common bandwidth, $h$, is chosen for both the outcome and the treatment, the estimated treatment effect is
\begin{align}
	\label{eq: frd estimator}
	\hat \zeta(h) 
	= \frac{\hat \tau_Y (h)}{\hat \tau_T (h)}
	= \frac{\hat \mu_{Y+}(h) - \hat \mu_{Y-}(h)}{\hat \mu_{T+}(h) - \hat \mu_{T-}(h)},
\end{align}
with
\begin{align*}
	\hat {\mu}_{Z+}(h)
	&= \argmin_{\beta_0} \min_{\beta_1} \sum_{i=1}^{n}
	\1\{X_{i} \geq 0\} (Z_{i} - \beta_0 - X_{i} \beta_1)^{2} \frac{1}{h} K(\frac{X_i}{h})
	\intertext{and}
	\hat {\mu}_{Z-}(h)
	&= \argmin_{\beta_0} \min_{\beta_1} \sum_{i=1}^{n}
	\1\{X_{i} < 0\} (Z_{i} - \beta_0 - X_{i} \beta_1)^{2}  \frac{1}{h} K(\frac{X_i}{h}).
\end{align*}

The conditional expectations $\mu_{Z+}$ and $\mu_{Z-}$ are consistently estimated by $\hat {\mu}_{Z+}(h)$ and $\hat {\mu}_{Z+}(h)$ when $h \to 0$.\footnote{Unless otherwise stated, all limits in this paper are assumed to hold as $n \to \infty$.} The asymptotic distribution of the quotient estimator $\frac{\hat \tau_Y (h)}{\hat \tau_T (h)}$ can be derived by applying the delta method. Let $V_Z$ be the asymptotic variance of $\hat \tau_Z(h)$ and $C_{YT}$ be the asymptotic covariance between  $\hat \tau_Y(h)$ and  $\hat \tau_T(h)$, i.e.,
\begin{equation*}
	\left({\begin{array}{*{20}{c}}
		{\sqrt {nh} (\hat \tau_Y(h) - \tau_Y) }\\
		{\sqrt {nh} (\hat \tau_T(h) - \tau_T) }
		\end{array}} \right) \to^d N
	\left( {\left( {\begin{array}{*{20}{c}}
		0\\
		0
		\end{array}} \right),\left( {\begin{array}{*{20}{c}}
		{{V_Y}}&{{C_{YT}}}\\
		{{C_{YT}}}&{{V_T}}
		\end{array}} \right)} \right),
\end{equation*}
then it follows that
\begin{equation*}
	\sqrt{nh} (\hat \zeta (h) - \zeta) \to^d N(0, \frac{1}{\tau_T^2} V_Y - \frac{2 \tau_Y}{\tau_T^3} C_{YT} + \frac{\tau_Y^2}{\tau_T^4} V_T).
\end{equation*}
Let $V(h) = \V (\hat \zeta(h) \mid X_1, ..., X_n)$, then $\frac{\hat \zeta(h) - \zeta}{\sqrt{V(h)}} \to^d N(0,1)$ and the confidence intervals can be constructed as
\begin{equation}
	\label{eq: frd CI}
	\hat{\zeta}(h) \pm q_{1-\alpha/2} V(h)^{1/2}
\end{equation}
where $q_{1 - \alpha/2}$ is the $1 - \alpha/2$ quantile of the standard normal distribution.

The above asymptotic distribution is valid only when bandwidth $h$ shrinks fast enough such that the bias of $\hat \zeta_Z(h)$ is negligible relative to $\sqrt{V(h)}$. Formally, $h=o_p(n^{-1/5})$ is required. With a bandwidth of order $O_p(n^{-1/5})$, \cite{hahn2001identification} show that the asymptotic distribution is normal but not centered at zero. Using \eqref{eq: frd CI} to construct confidence intervals without considering this first-order bias in distributional approximation leads to a coverage rate lower than the nominal level. \cite{imbens2011optimal} develop plug-in bandwidth selector for RD estimators, which is optimal in the sense that squared error loss of the point estimator is minimized. \cite{ludwig2005does} propose using cross validation to select bandwidth which minimizes squared prediction errors but find that the loss function is very ``flat" and leads to relatively large bandwidth. Neither of these two studies aims at searching optimal bandwidths which minimize the coverage error. 

Two different approaches are adopted in empirical studies. One is undersmoothing. In this case, instead of using the MSE-optimal bandwidth, researchers may want to choose a smaller bandwidth in order to meet the requirement of $h=o_p(n^{-1/5})$. However, this often leads to a series of ad-hoc bandwidths without theoretical basis. Another approach is bias correction. In this case, the leading bias is consistently estimated in an attempt to remove distortion of the asymptotic approximation. However, this approach does not perform well because the estimated bias introduces additional variability. The CCT's approach is based on bias correction, but redefines the variance component for normalization so that the additional variability is accounted for.

For any bandwidth $h \to 0$, the first-order bias of fuzzy RD estimator from local linear regression is
\begin{equation}
	\label{eq: bias}
	\E (\hat \zeta(h) \mid X_1, ..., X_n) - \zeta = h^2(\frac{1}{\tau_T} \B_Y(h) - \frac{\tau_Y}{\tau_T^2} \B_T(h))(1 + o_p(1)),
\end{equation}
with
\begin{equation*}
	\B_Z(h) = \frac{\mu_{Z+}^{(2)}}{2} \Bf_{+}(h) - \frac{\mu_{Z-}^{(2)}}{2} \Bf_{-}(h).
\end{equation*}
The terms $\Bf_{+}(h)$ and $\Bf_{-}(h)$, explicitly defined in appendix, are observed quantities that depend on the kernel, bandwidth and running variable. To explicitly calculate the first-order bias, one needs to estimate $\tau_Z$, $\mu_{Z+}^{(2)}$ and $\mu_{Z-}^{(2)}$. Among them $\tau_Z$ is consistently estimated by the local linear regression with bandwidth $h$. CCT propose a local second-order regression with a (potentially) different bandwidth $b$ to estimate the second order derivatives $\mu_{Z+}^{(2)}$ and $\mu_{Z-}^{(2)}$. This procedure gives the bias-corrected estimator
\begin{equation*}
	\hat \zeta^{bc}(h, b) = \hat \zeta (h) - \Delta(h,b),
\end{equation*}
with
\begin{align*}
	\Delta(h,b) & = h^2(\frac{1}{\hat \tau_T (h)} \hat \B_Y(h,b) - \frac{\hat \tau_Y (h)}{\hat \tau_T^2 (h)} \hat \B_T(h,b)), \\
	\hat \B_Z(h, b) & = \frac{\hat \mu_{Z+}^{(2)}(b)}{2} \Bf_{+}(h) - \frac{\hat \mu_{Z-}^{(2)}(b)}{2} \Bf_{-}(h).
\end{align*}
Notice that the bias $\Delta(h,b)$ is estimated with uncertainty. As a result, the variance of bias-corrected estimator $\hat \zeta^{bc}(h, b)$ is different from $V(h)$. CCT propose a new formula for the variance of bias-corrected estimator and use it for normalization:
\begin{equation}
	\label{eq: CCT asymptotic normality}
	\frac{\hat{\zeta}^{bc}(h, b) - \zeta}{V^{bc}(h, b)^{1/2}} \to^d N(0,1),
\end{equation}
where $V^{bc}(h, b) = V(h) + C(h, b)$ and $C(h, b)$ captures the adjustment to variance introduced by the bias-correction term. This distributional approximation is valid even when $h=O_p(n^{-1/5})$, as long as certain conditions on $h$ and $b$ are satisfied. Assumption \ref{as: bandwidth and kernel} specifies the bandwidth and kernel conditions assumed by CCT, which I will also use in this paper.
\begin{assumption}[Bandwidth and kernel]
	\label{as: bandwidth and kernel}
	Let $h$ be the bandwidth used to estimate the local linear model and let $b$ be the bandwidth used to estimate the second local quadratic model. Then $n h \to \infty$, $n b \to \infty$, and $n \times \min(h, b)^{5} \times \max(h, b)^2 \to 0$ as $n \to \infty$. The kernel function $K(\cdot)$ is positive, bounded, and continuous on the interval $[-\kappa,~\kappa]$ and zero outside that interval for some $\kappa > 0$.
\end{assumption}

Assumption \ref{as: bandwidth and kernel} does not require $nh^{1/5} \to 0$. Instead, it only requires that $nh^{1/5}b^{1/2} \to 0$ when $h < b$ or $nb^{1/5}h^{1/2} \to 0$ when $h > b$. This assumption also allows for the vast majority kernel functions commonly used in practice.

To simplify notation, let $m = \min(h,b)$ and define the scaled and truncated kernel functions
\begin{align*}
	K_{+,h}(x) &= \tfrac{1}{h} K(x/h) \1\{x \geq 0\} &
	K_{-,h}(x) &= \tfrac{1}{h} K(x/h) \1\{x < 0\}，
	\intertext{and}
	K_{+,b}(x) &= \tfrac{1}{b} K(x/b) \1\{x \geq 0\} &
	K_{-,b}(x) &= \tfrac{1}{b} K(x/b) \1\{x < 0\}.
\end{align*}

In the next section, a simple bootstrap procedure is proposed to construct robust confidence intervals based on the insight provided by CCT's bias-corrected estimator. This bootstrap procedure is straightforward in the sense that no derivation of analytical formulas for the bias, variance and covariance terms is required. The bias-corrected estimator and its confidence interval are numerically different from CCT's but asymptotically equivalent.

\section{Bootstrap algorithm}
\label{se: bootstrap algorithm}

In this section, two bootstrap algorithms are presented to obtain bias-corrected point estimator and its confidence intervals in the fuzzy RD designs. Their correctness is justified in two theorems and proved in the appendix. The idea behind both algorithms is to use local second-order polynomials to approximate the distribution of $(X_i, T_i, Y_i)$ around the cutoff. These second order polynomials, together with the variance structure, have known properties and act as the ``true" DGP as sample size increases. Assumption \ref{as: bandwidth and kernel} guarantees that the estimated ``true" DGP is close to the unknown DGP in the sense that distributional approximation derived from the ``true" DGP is asymptotically valid. This can be best illustrated from the special case where $h = b$, which translates to $nb^7 \to 0$ under Assumption \ref{as: bandwidth and kernel}. By the same argument that $h = o_p(n^{-1/5})$ is required for valid inference in a RD design estimated by local linear regression, $b = o_p(n^{-1/7})$ is required in a RD design estimated by local quadratic regression.

The first algorithm consistently estimates the bias term. In particular, after fitting local second-order regressions of outcome $Y_i$ and $T_i$ on running variable $X_i$ at each side of the cutoff, one can create many datasets through residual bootstrap. Each dataset generates a traditional fuzzy RD estimate, which are used to calculate the bias. Below is the detailed procedures in Algorithm \ref{al: bias estimation}.

\begin{algorithm}[Bias estimation]
	\label{al: bias estimation}
	Assume $h$ and $b$ are bandwidths as defined by Assumption~\ref{as: bandwidth and kernel}.
	\begin{enumerate}
		\item Estimate local second order polynomials $\hat g_{Z-}$ and $\hat g_{Z+}$
		with least squares using $K_{-,b}$ and $K_{+,b}$ for weights:
		\begin{align*}
		\hat g_{Z-}(x)
		&= \hat\beta_{Z-,0} + \hat\beta_{Z-,1} x + \hat\beta_{Z-,2} x^{2},
		&\hat g_{Z+}(x)
		&= \hat\beta_{Z+,0} + \hat\beta_{Z+,1} x + \hat\beta_{Z+,2} x^{2}
		\end{align*}
		with
		\begin{align*}
		(\hat\beta_{Z-,0}, \hat\beta_{Z-,1}, \hat\beta_{Z-,2})' &=
		\argmin_{\beta_0, \beta_1, \beta_2}
		\sum_{i = 1}^n (Z_i - \beta_0 - \beta_1 X_i - \beta_2 X_i^2)^{2} K_{-,b}(X_{i}) \\
		(\hat\beta_{Z+,0}, \hat\beta_{Z+,1}, \hat\beta_{Z+,2})' &= 
		\argmin_{\beta_0, \beta_1, \beta_2}
		\sum_{i = 1}^n (Z_i - \beta_0 - \beta_1 X_i - \beta_2 X_i^2)^{2} K_{+,b}(X_{i}).
		\end{align*}
		Let
		\[
		\hat g_Z(x) = \begin{cases}
		\hat g_{Z-}(x) & \textif\ x < 0 \\
		\hat g_{Z+}(x) & \otherwise
		\end{cases}
		\]
		and calculate the residuals $\hat\varepsilon_{Zi} = Z_i - \hat g_Z(X_i)$ for all $i$.
		\item Repeat the following steps $B_{1}$ times to produce the
		bootstrap estimates $\hat{\eta}_{1}^{*}(h),\dots,\hat\eta_{B_{1}}^{*}(h)$. For the
		$k$th replication:
		\begin{enumerate}
			\item Draw i.i.d.\ random variables $e_i^{*}$ with mean zero, variance one,
			and bounded fourth moments independent of the original data and
			construct
			\begin{align*}
			\varepsilon_{Zi}^{*} &= \hat{\varepsilon}_{Zi} e^{*}_i,
			\intertext{and}
			Z_i^{*} &= \hat g_Z(X_i) + \varepsilon_{Zi}^{*}
			\end{align*}
			for all $i$.
			\item Calculate $\hat\mu_{Z+}^*(h)$ and $\hat\mu_{Z-}^*(h)$ by estimating the
			local linear model on the bootstrap data set using $K_{+,h}$ and $K_{-,h}$ for
			weights:
			\begin{align*}
			\hat\mu_{Z-}^*(h)
			&= \argmin_{\mu} \min_{\beta} \sum_{i =1}^n
			(Z_i^* - \mu - \beta X_i)^2 K_{-,h}(X_{i}) \\
			\hat\mu_{Z+}^*(h)
			&= \argmin_{\mu} \min_{\beta} \sum_{i = 1}^n
			(Z_i^* - \mu - \beta X_i)^2 K_{+,h}(X_{i}).
			\end{align*}
			\item Save $\hat\zeta^*_k(h) = \frac{\hat\mu_{Y+}^*(h) - \hat\mu_{Y-}^*(h)}{\hat\mu_{T+}^*(h) - \hat\mu_{T-}^*(h)}$.
		\end{enumerate}
		\item Estimate the bias as
		\begin{equation}
		\label{eq: bootstrap bias estimator}
		\Delta^*(h,b) = \tfrac{1}{B_1} \sum_{k=1}^{B_1} \hat\zeta^*_k(h) -
		\frac{\hat g_{Y+}(0) - \hat g_{Y-}(0)}{\hat g_{T+}(0) - \hat g_{T-}(0)}.
		\end{equation}
	\end{enumerate}
\end{algorithm}

Algorithm \ref{al: bias estimation} consists of three steps. The first step estimates the bootstrap DGP, which is captured by second order local polynomials. The second step creates a series of new samples through wild bootstrap and finds the traditional fuzzy RD estimate for each sample. Notice that pairs of residuals are multiplied by the same realization of random number $e^*$ to preserve the correlation between $Y_i$ and $T_i$. The last step calculates the bias from local linear estimator by definition. Under Assumption \ref{as: DGP behavior}, \ref{as: bandwidth and kernel} and assume that $B_1$ is large enough, the procedure described by Algorithm \ref{al: bias estimation} gives a consistent estimator of the bias component that converges fast enough in probability that it can be used as a correction, resulting in a bias-corrected estimator that has the same asymptotic distribution as in \eqref{eq: CCT asymptotic normality}. This conclusion is formally given in Theorem \ref{th: bootstrap bias correction}.

\begin{theorem}
	\label{th: bootstrap bias correction}
	Under Assumptions \ref{as: DGP behavior} and \ref{as: bandwidth and kernel},
	\begin{equation}
	\label{eq: theorem bootstrap bias correction}
	\frac{\hat\zeta(h) - \Delta^{*}(h,b) - \zeta}{ V^{bc}(h, b)^{1/2}}
	\to^{d} N(0,1),
	\end{equation}
	where $\Delta^*(h,b)$ is defined by equation \eqref{eq: bootstrap bias estimator}.
\end{theorem}

Theorem \ref{th: bootstrap bias correction} enables one to construct valid confidence interval in the form of $\hat\zeta(h) - \Delta^{*}(h,b) \pm V^{bc}(h, b)^{1/2}$. However, the term $V^{bc}(h, b)$ still needs to be calculated. The second algorithm circumvents the analytical derivation of $V^{bc}(h, b)$ through an iterated bootstrap. In particular, the first layer bootstrap is designed to mimic the randomness due to sampling error and the second layer bootstrap, as described in Algorithm \ref{al: bias estimation}, is designed to estimate bias due to model misspecification. The additional variability introduced by the bias correction term will be automatically accounted for by this iterated bootstrap. The detailed procedure is given in Algorithm \ref{al: distribution}.

\begin{algorithm}[Distribution]
	\label{al: distribution}
	Assume $h$ and $b$ are bandwidths as defined by Assumption~\ref{as: bandwidth and kernel} and Algorithm~\ref{al: bias estimation}.
	\begin{enumerate}
		\item Estimate $\hat{g}_{Z+}$ and $\hat{g}_{Z-}$ and generate $\hat g_Z(\cdot)$ and the residuals $\hat\varepsilon_{Zi}$ just as in Algorithm~\ref{al: bias estimation}.
		\item Repeat the following steps $B_{2}$ times to produce bootstrap estimates of the bias-corrected estimate. For the $k$th replication:
		\begin{enumerate}
			\item Draw i.i.d.\ random variables $e_i^{*}$ with mean zero, variance one, and bounded fourth moments independent of the original data and
			construct
			\begin{align*}
			\varepsilon_{Zi}^{*} &= \hat{\varepsilon}_{Zi} e^{*}_i,
			\intertext{and}
			Z_i^{*} &= \hat g_Z(X_i) + \varepsilon_{Zi}^{*}.
			\end{align*}
			for all $i = 1,\dots,n$.
			\item Calculate $\hat\mu_{Z+}^*(h)$ and $\hat\mu_{Z-}^*(h)$ by estimating the
			local linear model on the bootstrap data set using $K_{+,h}$ and $K_{-,h}$
			for weights:
			\begin{align*}
			\hat\mu_{Z-}^*(h)
			&= \argmin_{\mu} \min_{\beta} \sum_{i = 1}^n
			(Z_{i}^{*} - \mu - \beta X_{i})^{2}K_{-,h}(X_{i}), \\
			\hat\mu_{Z+}^*(h)
			&= \argmin_{\mu} \min_{\beta} \sum_{i = 1}^n
			(Z_{i}^{*} - \mu - \beta X_{i})^{2}K_{+,h}(X_{i}).
			\end{align*}
			\item Apply Algorithm~\ref{al: bias estimation} to the bootstrapped data set $(X_1, T_1^*, Y_1^*),\dots,(X_n, T_n^*, Y_n^*)$ using the same bandwidths $h$ and $b$ that are used in the rest of this algorithm but reestimating all of the local polynomials on the bootstrap data. Generate $B_1$ new bootstrap samples and let $\Delta^{**}(h,b)$ represent the bias estimator returned by Algorithm~\ref{al: bias estimation}.
			\item Save the estimator $\hat\zeta_k^{*}(h) = \frac{\hat\mu_{Y+}^*(h) - \hat\mu_{Y-}^*(h)}{\hat\mu_{T+}^*(h) - \hat\mu_{T-}^*(h)}$, and its bias $\Delta_k^{**}(h,b)$.
		\end{enumerate}
		\item Define $\zeta^* = \frac{\hat g_{Y+}(0) - \hat g_{Y-}(0)}{\hat g_{T+}(0) - \hat g_{T-}(0)}$ and use the empirical CDF of $\hat\zeta_1^{*}(h) - \Delta_1^{**}(h,b)- \zeta^*,\dots, \hat\zeta_{B_2}^{*}(h)  - \Delta_{B_2}^{**}(h,b) - \zeta^*$ as the sampling distribution of $\hat \zeta(h) - \Delta^*(h,b) - \zeta$.
	\end{enumerate}
\end{algorithm}

Algorithm \ref{al: distribution} also consists of three steps. The first step estimates the bootstrap DGP, which is captured by second order local polynomials. The second step creates a series of new samples, to which the Algorithm \ref{al: bias estimation} is applied. The last step uses the empirical distribution of bias-corrected estimator to construct confidence intervals. As before, $B_2$ is assumed large enough so that simulation error can be ignored. The validity of Algorithm \ref{al: distribution} is established in the following theorem.

\begin{theorem}
	\label{th: bootstrap confidence intervals}
	Under Assumptions \ref{as: DGP behavior} and \ref{as: bandwidth and kernel},
	\begin{gather*}
		\V^*(\hat\zeta^{*}(h) - \Delta^{**}(h,b))/V^{bc}(h,b) \to^p 1
		\intertext{and}
		\sup_{x}
		\Bigg\rvert \Pr^*\Bigg[
		\frac{\hat\zeta^{*}(h) - \Delta^{**}(h,b) - \zeta^*}{\V^*(\hat\zeta^{*}(h) - \Delta^{**}(h,b))^{1/2}}
		\leq x \Bigg]
		- \Pr\Bigg[\frac{\hat\zeta(h) - \Delta^*(h,b) - \zeta}{V^{bc}(h,b)^{1/2}}
		\leq x \Bigg] \Bigg\lvert \to^p 0.
	\end{gather*}
\end{theorem}

Theorem \ref{th: bootstrap confidence intervals} enables one to construct confidence intervals in the following form:
\begin{equation*}
\big(\hat{\zeta} (h) - \Delta^*(h,b) + \zeta^* - (\hat\zeta^{*}(h) - \Delta^{**}(h,b))_{1-\alpha/2},
\hat{\zeta} (h) - \Delta^*(h,b) + \zeta^* + (\hat\zeta^{*}(h) - \Delta^{**}(h,b))_{\alpha/2} \big),
\end{equation*}
where all the terms with superscript $*$ are defined in Algorithm \ref{al: distribution}. Different from the analytical one, this confidence interval is not centered at the bias-corrected point estimator. Several remarks on implementing these algorithms are listed below.

\begin{remark}
	The proposed bias correction differs from CCT's analytical formula in finite sample. While the analytical bias is obtained by firstly linearizing $\E \big(\frac{\hat{\tau}_Y (h)}{\hat{\tau}_T (h)} - \frac{\tau_Y}{\tau_T} \big)$ and then only evaluating its first order terms, Algorithm \ref{al: bias estimation} directly estimates $\E \big(\frac{\hat{\tau}_Y^* (h)}{\hat{\tau}_T^* (h)} - \frac{\tau_Y^*}{\tau_T^*}\big) $ through bootstrap. Both methods consistently estimate the bias.
\end{remark}

\begin{remark}
	When the original treatment is binary, the bootstrap sample will no longer have binary treatment. Though it creates some difficulty for interpretation, it does not invalidate the estimation and inference because its conditional expectation and covariance with outcome variable remain unchanged.
\end{remark}

\begin{remark}
	The iterated bootstrap is less computationally intensive than it appears to be because of two reasons. First, the wild bootstrap creates new residuals but leaves the regressors unchanged, which means the design matrices only need to be computed once even when they are repeatedly used in fitting local polynomials.\footnote{To fit local polynomials is equivalent to estimate weighted least square, i.e., the estimated parameter is $(\mathbf{X}' \mathbf{K} \mathbf{X})^{-1} \mathbf{X}' \mathbf{K} \mathbf{Y}$, where $\mathbf{X}$ is matrix of regressors and $\mathbf{K}$ is weighting matrix determined by kernel function. Both $\mathbf{X}$ and $\mathbf{K}$ are not affected by the bootstrap so one just need to compute $(\mathbf{X}' \mathbf{K} \mathbf{X})^{-1} \mathbf{X}' \mathbf{K}$ once and then reused it in the bootstrap calculations. Then each bootstrap replication just requires a single matrix-vector multiplication.} Second, the number of data points actually used in estimation is a lot smaller than the full sample. 
\end{remark}

The bootstrap procedure used in these two algorithms is in line with conventional bootstrap procedure for IV regression. When generating new samples from an IV model using residual bootstrap, one usually firstly estimates both reduced equation and structural equation and then randomly draws residual pairs from these two equations. Here in the fuzzy RD designs, two reduced equations are estimated. Instead of randomly drawing residual pairs, wild bootstrap is adopted to accommodate potential heteroskedasticity.

Both the CCT's approach and the bootstrap approach presented above are robust to bandwidth choice, in the sense that traditional MSE-optimal bandwidth is allowed for valid inference, but they are not robust to weak identification.\footnote{As a measurement of the strength of instrumental variable, the concentration parameter in the setting of fuzzy RD designs is determined by the effective sample size ($nh$), density of the running variable at the cutoff ($f(0)$), variance of the treatment variable at the cutoff ($\sigma_{T-}^2(0), \sigma_{T+}^2(0)$) and discontinuity in treatment probability ($\mu_{T+}(0) - \mu_{T-}(0)$) \citep{feir2016weak}.} The wild bootstrap requires an initial estimation of the model, based on which resampling is conducted. Weak instrument makes it difficult to precisely estimate the model and thus the approximation to the true data generating process is poor. Alternatives which can improve performance include imposing the NULL hypothesis or bootstrapping (asymptotically) pivotal statistics \citep{davidson2008wild,cameron2008bootstrap}. Statistical inference from the two analytical methods relies on the assumption that the estimate is asymptotically normal, which is likely to be very skewed when identification is weak.

Evidence of the usefulness of the new procedure proposed above and its relative performance to the analytical bias correction proposed in CCT are presented in a series of Monte Carlo simulations in Section \ref{se: simulation}.

\section{Simulation}
\label{se: simulation}

The proposed bootstrap algorithms are applied to a variety of data generating processes (DGP). The baseline DGP is similar to CCT but re-designed to fit the fuzzy RD designs:

\begin{align*}
X_i & \sim  2 \times \textit{beta}(2,4) - 1 \\
T_i & = \1 \{u_{ti} \le \Phi^{-1} (0.5 - \frac{c}{2}) \}  \1 \{ X_i < 0\} + 
\1 \{u_{ti} \le \Phi^{-1} (0.5 + \frac{c}{2}) \}  \1 \{ X_i \ge 0\} \\
Y_i & = \mu_j (X_i) + T_i \zeta_j + u_{yi},
\end{align*}
where $u_{ti} \sim N(0, 1)$ and $c = 0.9$. The equation for $T_i$ indicates that $\mu_{T-} = 0.5 - c/2$ and $\mu_{T+} = 0.5 + c/2$. As a result, the expected treatment conditional on running variable is constant on both sides but the discontinuity at the cutoff is exactly $c$. In the equation for $Y_i$, the first part on the right, $\mu_j(X_i)$ with $j = 1, 2, 3,$ is the conditional expected outcome without treatment, which is continuous at the cutoff. The second part on the right, $T_i \zeta_j$, captures the additive treatment effect. In particular, the conditional expected outcome takes the following forms:

\begin{align*}
\mu_{1}(x) & =
\begin{cases}
1.27x + 7.18x^{2} + 20.21x^{3} + 21.54x^{4} + 7.33x^{5}
& \textif\ x < 0 \\
0.84x - 3.00x^{2} + 7.99x^3 - 9.01x^4 + 3.56x^{5}
& \otherwise,
\end{cases} \\
\mu_{2}(x) & =
\begin{cases}
2.30x + 3.28x^2 + 1.45x^3 + 0.23x^4 + 0.03x^5
& \textif\ x < 0, \\
18.49x - 54.81x^2 + 74.30x^3 - 45.02x^4 + 9.83x^5
& \otherwise,
\end{cases} \\
\mu_{3}(x) & =
\begin{cases}
1.27x + 3.59 x^{2} + 14.147 x^3 + 23.694 x^4 + 10.995 x^5
& \textif\ x < 0 \\
0.84x - 0.30 x^{2} + 2.397 x^3 - 0.901 x^4 + 3.56 x^5
& \otherwise.
\end{cases}
\end{align*}

These conditional mean functions are adapted from DGPs for sharp RD designs by preserving the curvature but removing the discontinuity at the cutoff. The first mean function is designed to match features of U.S. congressional election data \citep{lee2008randomized,imbens2011optimal}. The second mean function is designed to match the relation between children mortality rate and county poverty rate from analysis of Head Start data \citep{ludwig2005does}. The last mean function is similar to the first one except for some coefficients. CCT motivates this in an attempt to generate plausible model with sizable distortion when conventional t-test is performed. The true treatment effects for these three models are $\zeta_1 = 0.04, \zeta_2 = -3.45, \zeta_3 = 0.04$.

To accommodate a variety of different error structure in empirical data, the following three cases are considered.

\begin{enumerate}
	\item Baseline case. The simplest case where errors are independently and identically distributed:
		\begin{equation*}
		\left( {\begin{array}{*{20}{c}}
			{{u_{ti}}}\\
			{{u_{yi}^*}}
			\end{array}} \right) \sim N\left( {\left( {\begin{array}{*{20}{c}}
				0\\
				0
				\end{array}} \right),\left( {\begin{array}{*{20}{c}}
				1&\rho \\
				\rho &1
				\end{array}} \right)} \right), \;\;
			\rho = 0, \;\;
			u_{yi} = 0.1295 u_{yi}^*.
		\end{equation*}
	\item Heteroskedasticity. The disturbance term in the outcome equation has a standard error changing with the running variable, i.e., everything being the same as in the baseline case except $u_{yi} = (0.1295 + 9 x_i^2) u_{yi}^*$.\footnote{The motivation is to keep the standard error unchanged from the homoskedastic case at the cutoff, so that estimators from these two cases are more comparable in the sense that they are equivalent in the limit.}

	\item Endogeneity. The treatment status is correlated with unobserved characteristics which affect the outcome. This is modeled by the correlation between $u_{ti}$ and $u_{yi}$, i.e., everything being the same as in the baseline case except $\rho \in \{-0.9, 0.9\}$.
\end{enumerate} 

In the implementation of Algorithm \ref{al: bias estimation} and \ref{al: distribution}, the two-point distribution proposed in \cite{mammen1993bootstrap} is adopted for creating bootstrap samples. This auxiliary distribution is
\begin{equation*}
e_i^* =
\begin{cases}
\frac{1 + \sqrt{5}}{2} & \text{with probability\ } \frac{\sqrt{5} - 1}{2 \sqrt{5}}, \\
\frac{1 - \sqrt{5}}{2} & \text{otherwise},
\end{cases}
\end{equation*}
with zero mean and unit second and third moments. Its property ensures that the skewness of the bootstrap error terms is the same as the skewness of the residuals, which is a desirable condition not imposed in Algorithm \ref{al: bias estimation} and \ref{al: distribution}.\footnote{Some later studies also show good properties of the simpler Rademacher distribution \citep{flachaire2005bootstrapping,davidson2008wild}.} In addition, the residuals are transformed before applying bootstrap because they are on average underestimated by least squares. Specifically, instead of directly using $\hat{\varepsilon}_{Zi}$, the ``HC3" type transformation $\hat{\varepsilon}_{Zi}/(1 - H_{ii})$ is applied, with $H_{ii}$ being the diagonal element of projection matrix.\footnote{Local regressions project $\mathbf{K}^{1/2} \mathbf{Y}$ onto space of $\mathbf{K}^{1/2} \mathbf{X}$, with $\mathbf{K}$ being the weighting matrix determined by kernel function. So the projection matrix will be $\mathbf{K}^{1/2} \mathbf{X} (\mathbf{X}' \mathbf{K} \mathbf{X})^{-1} \mathbf{X}' \mathbf{K}^{1/2}$.} This is based on jackknife covariance estimator and is shown to outperform the original heteroskedasticity-robust covariance estimator \citep{mackinnon1985some}. Simulation studies by \cite{davidson2008wild} and \cite{mackinnon2013thirty} also provide some evidence in favor of ``HC3" transformation.

The bootstrap approach uses $B_1 = 500$ replications to compute bias and $B_2 = 999$ replications to obtain empirical distribution of bias-corrected estimator. Besides the bootstrap approach, two additional approaches are estimated for comparison: the CCT's robust estimator and the conventional estimator.\footnote{All simulations are conducted with R software. Packages \textit{rdrobust (V0.94)} and \textit{RDD (V0.57)} are used to estimate the CCT's robust estimator and conventional RD estimator respectively. By default, the former one uses the nearest neighbor variance estimator and the latter one uses ``HC1" type heteroskedasticity-robust variance estimator.} The two bandwidths for the bootstrap approach and the CCT's approach are the same and are obtained by utilizing bandwidth selector from CCT. The bandwidth used in the conventional approach is chosen by MSE-optimal bandwidth selector proposed by \cite{imbens2011optimal}.\footnote{As is suggested by \cite{imbens2011optimal}, the optimal bandwidth choices in fuzzy RD designs are often similar to those based on the optimal bandwidth for the numerator only. For simplicity, all bandwidths are calculated ignoring the fact that the RD design is fuzzy.} These three approaches are applied to a total number of 5000 simulated samples with a sample size of 1000. Triangular kernel is used throughout all the simulations in this paper.\footnote{Results with other kernel functions are similar and available in a separate document.}

Simulation results are shown in Table \ref{tb: basic simulation} and \ref{tb: endogeneity simulation}. For the estimated treatment effect, its bias, standard error and root of mean squared error are reported in the first three columns. For the confidence interval, its empirical coverage and average length are reported in the fourth and fifth columns. The last three columns list the bandwidths used in the two robust methods ($h_{CCT}, b_{CCT}$) and the conventional method ($h_{IK}$). For each sample, the wild bootstrap approach uses the same bandwidths as the CCT's robust approach.

\begin{table}[ht]
	\begin{threeparttable}
		\caption{Empirical coverage and average interval length}
		\label{tb: basic simulation}
		\begin{tabular}{llrrrrrrrr}
			\hline
			DGP & Method & Bias & SD & RMSE & EC(\%) & IL & $h_{CCT}$ & $b_{CCT}$ & $h_{IK}$ \\ 
			\hline
			&& \multicolumn{8}{c}{Panel A: homoskedastic data} \\ \cline{3-10}
			1 & Wild bootstrap & 0.015 & 0.054 & 0.056 & 93.1 & 0.197 & 0.197 & 0.323 &  \\ 
			& CCT robust & 0.015 & 0.054 & 0.056 & 91.5 & 0.191 & 0.197 & 0.323 &  \\ 
			& Conventional & 0.042 & 0.032 & 0.053 & 68.1 & 0.116 &  &  & 0.400 \\ 
			2 & Wild bootstrap & 0.037 & 0.058 & 0.069 & 86.9 & 0.210 & 0.165 & 0.299 &  \\ 
			& CCT robust & 0.039 & 0.060 & 0.071 & 86.6 & 0.212 & 0.165 & 0.299 &  \\ 
			& Conventional & 0.215 & 0.079 & 0.229 & 2.6 & 0.186 &  &  & 0.216 \\ 
			3 & Wild bootstrap & 0.005 & 0.053 & 0.053 & 95.3 & 0.205 & 0.162 & 0.317 &  \\ 
			& CCT robust & 0.005 & 0.053 & 0.054 & 94.1 & 0.200 & 0.162 & 0.317 &  \\ 
			& Conventional & -0.025 & 0.044 & 0.050 & 87.3 & 0.157 &  &  & 0.205 \\ 
			
			&& \multicolumn{8}{c}{Panel B: heteroskedastic data} \\ \cline{3-10}
			1 & Wild bootstrap & 0.004 & 0.078 & 0.079 & 95.8 & 0.294 & 0.110 & 0.189 &  \\ 
			& CCT robust & 0.004 & 0.071 & 0.071 & 94.0 & 0.268 & 0.110 & 0.189 &  \\ 
			& Conventional & 0.029 & 0.048 & 0.057 & 90.8 & 0.185 &  &  & 0.237 \\ 
			2 & Wild bootstrap & 0.028 & 0.066 & 0.072 & 92.9 & 0.255 & 0.149 & 0.259 &  \\ 
			& CCT robust & 0.030 & 0.067 & 0.073 & 91.3 & 0.251 & 0.149 & 0.259 &  \\ 
			& Conventional & 0.232 & 0.109 & 0.256 & 5.8 & 0.213 &  &  & 0.226 \\ 
			3 & Wild bootstrap & 0.001 & 0.069 & 0.069 & 96.2 & 0.294 & 0.110 & 0.190 &  \\ 
			& CCT robust & 0.001 & 0.069 & 0.069 & 94.4 & 0.267 & 0.110 & 0.190 &  \\ 
			& Conventional & -0.039 & 0.061 & 0.072 & 83.0 & 0.187 &  &  & 0.230 \\ 
			\hline
		\end{tabular}
		\begin{tablenotes}
			\small
			\item Note: EC denotes empirical coverage and IL denote average interval length based on 5000 simulations; nominal coverage probabilities are 95\% for each estimator. The columns $h_{CCT}$ and $b_{CCT}$ list average optimal bandwidths following CCT's method.The column $h_{IK}$ lists average optimal bandwidth minimizing MSE.
		\end{tablenotes}
	\end{threeparttable}
\end{table}

Table \ref{tb: basic simulation} presents these results for data with and without heteroskedasticity. The baseline case is listed in Panel A. The two robust methods, wild bootstrap and CCT's approach, generate point estimates with very similar bias and standard error (identical for DGP 1 and 3 and slightly different for DGP 2). In contrast, the conventional approach reports 3-5 times larger bias. This is not surprising since the two robust methods explicitly correct the bias. The conventional method also fails to deliver a valid interval (coverage rates are 68.1\%, 2.6\% and 87.2\% for the three DGPs respectively). Improvement is achieved by the robust methods by reducing bias and increasing interval length. Except for DGP 2, they both generate intervals with empirical coverage close to the nominal level and the wild bootstrap is lightly better (93.1\% VS 91.5\% for DGP 1 and 95.3\% VS 94.1\% for DGP 3). However, for DGP 2, even the robust methods report great size distortion. This is because DGP 2 has great curvature around the cutoff and makes precise fitting very difficult.\footnote{In particular, the DGP 2 shows great curvature just right to the cutoff. On the right side, its second derivative at the cutoff is -109.62, so local linear regression is likely to create large bias. Its third derivative at the cutoff is 445.8, so local quadratic regression is likely to create large bias.} Still, the two robust methods improve significantly from the conventional method in coverage (from 2.6\% to around 87\%) at the sacrifice of slightly longer intervals (from 0.186 to around 0.21).

Panel B in Table \ref{tb: basic simulation} lists the results when the data is heteroskedastic.\footnote{In Panel A where homoskedastic DGP is used, there still exists heteroskedasticity from the perspective of estimation due to model specification, i.e., to use polynomials with order lower than the true one.} A significant difference from the homoskedastic case is the bandwidth choice. For the two robust methods, the bandwidths are reduced from the homoskedastic case while for the conventional method, this happens only to DGP 1. The increased noise in the data may reduce the perceived curvature by bandwidth selector and thus a smaller bandwidth is picked. Smaller bandwidth leads to smaller bias and larger variance. As a result, intervals in Panel B have higher coverage rate with longer interval length. The overall pattern in Panel B is similar to Panel A because all the three methods are robust to heteroskedasticity.

Table \ref{tb: endogeneity simulation} presents results when the treatment is endogenous, which is almost always true and probably the primary reason to choose RD designs as the identification strategy. The case with positive self-selection is listed in Panel A and negative self-selection in Panel B. Again, the estimate from conventional method has significantly larger bias than the other two robust methods. As for interval estimates, the wild bootstrap and the CCT's approach work reasonably well in all cases except for DGP 2, where the empirical coverage is around 90\% with positive self-selection and 85\% with negative self-selection. The conventional method performs significantly worse, with empirical coverage rate as low as 1.7\% (DGP 2 with negative self-selection). The sign of correlation has little effect on the bias because the bias is caused by model misspecification rather than imperfect instrumental variable.

\begin{table}[ht]
	\begin{threeparttable}
		\caption{Empirical coverage and average interval length (endogenous treatment)}
		\label{tb: endogeneity simulation}
		\begin{tabular}{llrrrrrrrr}
			\hline
			DGP & Method & Bias & SD & RMSE & EC(\%) & IL & $h_{CCT}$ & $b_{CCT}$ & $h_{IK}$ \\ 
			\hline
			&& \multicolumn{8}{c}{Panel A: $\rho = 0.9$} \\ \cline{3-10}
			1 & Wild bootstrap & 0.016 & 0.054 & 0.056 & 95.7 & 0.203 & 0.197 & 0.323 &  \\ 
			& CCT robust & 0.017 & 0.055 & 0.057 & 93.1 & 0.196 & 0.197 & 0.323 &  \\ 
			& Conventional & 0.043 & 0.033 & 0.054 & 70.7 & 0.121 &  &  & 0.398 \\ 
			2 & Wild bootstrap & 0.037 & 0.064 & 0.074 & 90.4 & 0.220 & 0.168 & 0.302 &  \\ 
			& CCT robust & 0.041 & 0.067 & 0.078 & 89.7 & 0.233 & 0.168 & 0.302 &  \\ 
			& Conventional & 0.226 & 0.092 & 0.244 & 3.0 & 0.207 &  &  & 0.222 \\ 
			3 & Wild bootstrap & 0.004 & 0.062 & 0.062 & 95.9 & 0.214 & 0.161 & 0.316 &  \\ 
			& CCT robust & 0.007 & 0.055 & 0.056 & 94.8 & 0.202 & 0.161 & 0.316 &  \\ 
			& Conventional & -0.024 & 0.043 & 0.049 & 86.5 & 0.156 &  &  & 0.204 \\ 
			
			&& \multicolumn{8}{c}{Panel B: $\rho = -0.9$} \\ \cline{3-10}
			1 & Wild bootstrap & 0.015 & 0.053 & 0.056 & 91.3 & 0.198 & 0.199 & 0.324 &  \\ 
			& CCT robust & 0.013 & 0.055 & 0.056 & 91.1 & 0.190 & 0.199 & 0.324 &  \\ 
			& Conventional & 0.042 & 0.031 & 0.052 & 65.7 & 0.113 &  &  & 0.402 \\ 
			2 & Wild bootstrap & 0.037 & 0.052 & 0.064 & 85.5 & 0.205 & 0.161 & 0.296 &  \\ 
			& CCT robust & 0.038 & 0.052 & 0.064 & 84.4 & 0.190 & 0.161 & 0.296 &  \\ 
			& Conventional & 0.201 & 0.064 & 0.211 & 1.7 & 0.165 &  &  & 0.208 \\ 
			3 & Wild bootstrap & 0.005 & 0.053 & 0.053 & 95.6 & 0.206 & 0.163 & 0.317 &  \\ 
			& CCT robust & 0.003 & 0.054 & 0.054 & 94.5 & 0.203 & 0.163 & 0.317 &  \\ 
			& Conventional & -0.027 & 0.045 & 0.052 & 89.1 & 0.160 &  &  & 0.207 \\ 
			\hline
		\end{tabular}
		\begin{tablenotes}
			\small
			\item Note: EC denotes empirical coverage and IL denote average interval length based on 5000 simulations; nominal coverage probabilities are 95\% for each estimator. The columns $h_{CCT}$ and $b_{CCT}$ list average optimal bandwidths following CCT's method.The column $h_{IK}$ lists average optimal bandwidth minimizing MSE.
		\end{tablenotes}
	\end{threeparttable}
\end{table}

To summarize, the wild bootstrap approach proposed in this paper performs significantly better than the conventional method and is at least on par with the CCT's analytical methods. This wild bootstrap procedure automatically accommodate various types of covariance structure and thus is a simple alternative to obtain valid confidence intervals in RD designs. 

\section{Extension: clustered data}

This section explores the application of the bootstrap procedure to clustered data in RD designs and provides evidence for its usefulness. Clustered data are very common in empirical studies. Units within the same cluster are usually dependent and ignoring this dependence is likely to invalidate statistic inference. There is enormous literature on handling clustered data.\footnote{Specifically, see \cite{wooldridge2003cluster,cameron2012robust,cameron2015practitioner} for an overview on this topic.} In short, one can either explicitly estimate the dependence structure with some additional specifications, such as random coefficient models, or account for the dependence after estimation, such as using cluster-robust variance estimator \citep{liang1986longitudinal,arellano1987practitioners}.

To use cluster-robust variance estimator in statistical inference is very popular partly because it does not require assumption on the dependence structure and partly because its availability in almost all statistical software. Its validity is based on asymptotics when the number of clusters grows to infinity, which is, unfortunately, not trivial to establish in nonparametric models. The main obstacle is that shrinking tunning variable is likely to destroy the dependence structure. For local polynomial regressions, \cite{wang2003marginal} and \cite{chen2008design} point out that the existence of joint density of running variable and clustering variable ensures that all clusters will eventually include only a single unit as the bandwidth shrinks to zero. As a result, the clustering structure disappears. A special case where this does not happen is that clustering occurs at the running variable level \citep{chen2005local}.\footnote{For example, in panel data where each individual are observed for multiple times and the running variable is at individual level, each individual is a cluster and will not vanish with shrinking bandwidth. \cite{lee2008regression} consider another example in RD designs where clustering occurs at the running variable level and cluster-robust variance estimator is recommended in inference.} This is exactly the case for which \cite{bartalotti2016regression} develop optimal bandwidth selector.

Though there is no asymptotics specifically developed for general RD designs with clustered data, currently available softwares usually provide options to take this dependence into consideration.\footnote{For example, both the \textit{rdrobust} and \textit{RDD} packages used in this paper offer the option to specify a clustering variable.} After all, the estimation of RD designs is no different from linear regression once the bandwidth is given and conventional cluster-robust variance estimator can be easily applied. Bootstrap is also known to be applicable to clustered data. \cite{cameron2008bootstrap} provide a comprehensive survey of bootstrap method and show that proper bootstrap procedures outperform the conventional cluster-robust variance estimator when the number of clusters is small (five to thirty).

To check the flexibility and robustness of wild bootstrap procedure proposed in this paper, I slightly revise the resampling algorithm to accommodate clustering and test its performance with pseudo clustered data. Following \cite{brownstone2001bootstrap} and \cite{cameron2008bootstrap}, the wild bootstrap procedure for clustered data is quite straightforward: for units in the same cluster, their residuals are multiplied by the same random number drawn from the auxiliary distribution. For example,

\begin{equation*}
Z_{gi}^{*} = \hat g_Z(X_{gi}) + \hat{\varepsilon}_{Z_{gi}} e_g^*,
\end{equation*}
where $e_g^*$, a random number from distribution with zero mean and unit variance, is shared by all units in the same group. For the purpose of simulation, it is assumed that errors in the outcome equation is clustered according to a random effect model, in particular, 

\begin{equation*}
u_{ygi} = u_{yg}^* + u_{yi}^*, \;\; u_{yg}^*, u_{yi}^* \sim i.i.d. \; N(0, \frac{0.1295}{\sqrt{2}}),
\end{equation*}
with $g = 1, 2, \dots, G$ being a group indicator. This design ensures that each individual error has a standard error of 0.1295, which is the same as the baseline case. However, half of its variability is contributed by a random effect at the group level.

Simulation results for $G = 5, 10, 25$ are reported in Table \ref{tb: cluster simulation}.\footnote{Since the RD designs is estimated separately on each side, $G$ means the number of clusters on each side. It is assumed there is no clusters crossing the cutoff.} Again, two other methods besides the wild bootstrap method are estimated.\footnote{For the conventional method, since there is no bandwidth selector specifically designed for general clustered data (the one proposed by \cite{bartalotti2016regression} does not apply), I use the MSE-optimal bandwidth selector ignoring the fact that data is actually clustered. The conventional method uses cluster-robust variance estimator to construct confidence interval. For the CCT's robust approach, I used their companion R package \textit{rdrobust}, which accommodates clustered data in both bandwidth selection and interval construction.} All the three methods fail to give a good interval estimate, which are well below the nominal level. This is not surprising because interval estimates from the two analytical methods (the CCT's roust approach and the conventional method) are based on large $G$ asymptotics. The wild bootstrap approach consistently performs better than the conventional method, but does not improve much from the CCT's robust approach. The wild bootstrap procedure proposed in this paper is similar to the ``wild bootstrap-se" method considered by \cite{cameron2008bootstrap}. Their simulation results show that ``wild bootstrap-se" method still suffers from size distortion with small number of clusters and is inferior to ``wild bootstrap-t" method. The ``wild bootstrap-t" method works well because (1) it imposes the NULL hypothesis so that estimation is more precise and (2) it bootstraps asymptotically pivotal t-statistics and achieves refinement. 

This simple experiment shows that the wild bootstrap procedure can not only give valid confidence interval with independent data, it can also be easily applied to clustered data with slight adjustment to its resampling algorithm and performs at least as good as the analytical robust method.

\begin{table}
	\begin{threeparttable}
		\caption{Empirical coverage and average interval length (clustered data)}
		\label{tb: cluster simulation}
		\begin{tabular}{llrrrrrrrr}
			\hline
			DGP & Method & Bias & SD & RMSE & EC(\%) & IL & $h_{CCT}$ & $b_{CCT}$ & $h_{IK}$ \\ 
			\hline
			&& \multicolumn{8}{c}{Panel A: $G = 5$} \\ \cline{3-10}
			1 & Wild bootstrap & 0.018 & 0.081 & 0.083 & 87.0 & 0.268 & 0.251 & 0.318 &  \\ 
			& CCT robust & 0.018 & 0.081 & 0.083 & 86.8 & 0.274 & 0.251 & 0.318 &  \\ 
			& Conventional & 0.043 & 0.071 & 0.083 & 83.7 & 0.249 &  &  & 0.392 \\ 
			2 & Wild bootstrap & 0.037 & 0.085 & 0.093 & 83.4 & 0.274 & 0.165 & 0.297 &  \\ 
			& CCT robust & 0.039 & 0.086 & 0.094 & 84.0 & 0.289 & 0.165 & 0.297 &  \\ 
			& Conventional & 0.214 & 0.101 & 0.237 & 22.5 & 0.275 &  &  & 0.216 \\ 
			3 & Wild bootstrap & 0.007 & 0.080 & 0.080 & 88.6 & 0.270 & 0.200 & 0.312 &  \\ 
			& CCT robust & 0.007 & 0.080 & 0.081 & 89.0 & 0.276 & 0.200 & 0.312 &  \\ 
			& Conventional & -0.023 & 0.076 & 0.080 & 87.5 & 0.261 &  &  & 0.202 \\ 
			
			&& \multicolumn{8}{c}{Panel B: $G = 10$} \\ \cline{3-10}
			1 & Wild bootstrap & 0.017 & 0.068 & 0.070 & 90.2 & 0.240 & 0.230 & 0.321 &  \\ 
			& CCT robust & 0.018 & 0.068 & 0.071 & 88.9 & 0.236 & 0.230 & 0.321 &  \\ 
			& Conventional & 0.043 & 0.055 & 0.070 & 83.8 & 0.200 &  &  & 0.396 \\ 
			2 & Wild bootstrap & 0.036 & 0.071 & 0.079 & 87.1 & 0.250 & 0.166 & 0.299 &  \\ 
			& CCT robust & 0.038 & 0.071 & 0.081 & 86.2 & 0.253 & 0.166 & 0.299 &  \\ 
			& Conventional & 0.213 & 0.089 & 0.231 & 12.9 & 0.239 &  &  & 0.216 \\ 
			3 & Wild bootstrap & 0.005 & 0.067 & 0.067 & 92.7 & 0.243 & 0.186 & 0.316 &  \\ 
			& CCT robust & 0.005 & 0.068 & 0.068 & 91.5 & 0.240 & 0.186 & 0.316 &  \\ 
			& Conventional & -0.025 & 0.062 & 0.067 & 88.0 & 0.220 &  &  & 0.204 \\ 
			
			&& \multicolumn{8}{c}{Panel C: $G = 25$} \\ \cline{3-10}
			1 & Wild bootstrap & 0.016 & 0.061 & 0.063 & 91.7 & 0.216 & 0.213 & 0.323 &  \\ 
			& CCT robust & 0.016 & 0.061 & 0.063 & 89.6 & 0.210 & 0.213 & 0.323 &  \\ 
			& Conventional & 0.043 & 0.043 & 0.060 & 78.7 & 0.157 &  &  & 0.399 \\ 
			2 & Wild bootstrap & 0.038 & 0.065 & 0.075 & 86.8 & 0.228 & 0.165 & 0.300 &  \\ 
			& CCT robust & 0.040 & 0.066 & 0.077 & 86.6 & 0.230 & 0.165 & 0.300 &  \\ 
			& Conventional & 0.214 & 0.084 & 0.230 & 6.4 & 0.210 &  &  & 0.216 \\ 
			3 & Wild bootstrap & 0.004 & 0.060 & 0.060 & 94.1 & 0.221 & 0.174 & 0.317 &  \\ 
			& CCT robust & 0.004 & 0.060 & 0.061 & 92.6 & 0.216 & 0.174 & 0.317 &  \\ 
			& Conventional & -0.025 & 0.053 & 0.059 & 86.6 & 0.186 &  &  & 0.205 \\ 
			\hline
		\end{tabular}
		\begin{tablenotes}
			\small
			\item Note: EC denotes empirical coverage and IL denote average interval length based on 5000 simulations; nominal coverage probabilities are 95\% for each estimator. The columns $h_{CCT}$ and $b_{CCT}$ list average optimal bandwidths following CCT's method.The column $h_{IK}$ lists average optimal bandwidth minimizing MSE.
		\end{tablenotes}
	\end{threeparttable}
\end{table}

\section{Application}

In this section, I apply the bootstrap procedure to the data used in \cite{angrist1999using}.\footnote{The data is available at http://economics.mit.edu/faculty/angrist/data1/data/anglavy99.} In their paper, the effects of class size on scholastic achievement are estimated using the Maimonides' rule as instrument.

Maimonides, a great Rabbinic scholar, holds the view that the maximum class size is 40. This rule has been adopted by Israeli public schools to determine the division of enrollment cohorts into classes since 1969. Following this rule, when the enrollment increases and passes multiples of 40, an additional class is required. Since the total enrollment is roughly evenly divided into all classes, an additional class causes a sudden drop in class size. Ideally, when the enrollment grows from 40 to 41, class size will drop by half. Because of student turnover and imperfect enforcement of this rule, the empirical data fit into a fuzzy RD design.

The first discontinuity in class size for the 4th grade is considered. The sample used in this application includes 1164 classes from schools with enrollments no larger than 80. The outcome variables are average verbal and math test scores at class level. The discontinuities in class size and outcomes against enrollment are visualized in Figure \ref{fig: discontinuity}.
\begin{figure}[t]
	\small
	\centering
	\includegraphics[scale = 0.42]{application/final4_sub.pdf}
	\caption{Class size, average verbal and math scores}
	\label{fig: discontinuity}
\end{figure} 
Each dot in these plots represents a class and the regression lines are fitted by fourth order polynomials. The shaded areas indicate confidence interval. The first plot clearly shows the discontinuity in class size. The second plot suggests a discontinuity in average verbal score, but not as significant as that in class size. The last plot does not provide much evidence for a discontinuity in average math score. 

Similar to the simulations in Section \ref{se: simulation}, three methods are applied to estimate the effect of class size on average verbal/math scores and results are shown in Table \ref{tb: the effect of class size}. The first column lists the original point estimates from local linear regression, which depends only on the bandwidth choice. This explains why estimates from wild bootstrap and CCT's approach are identical and they are close to the conventional estimate. The second column lists the bias-corrected point estimates based on bootstrap bias correction and analytical bias correction. They are very close to each other but differ a lot from the original estimates (the magnitude increases from 0.449 to 0.575 for average verbal score and 0.185 to 0.263$\sim$0.272 for average math score).

Consistent with what Figure \ref{fig: discontinuity} shows, only one out of three intervals for the treatment effect on average verbal score excludes zero and all three intervals for the treatment on average match score include zero. The interval from wild bootstrap is wider than that from robust analytical approach, suggesting that it is more conservative, which also can be found from previous simulation studies.

\begin{table}[t]
	\centering
	\begin{tabular}{lrrrrrrr}
		\hline
		&\multicolumn{2}{c}{ATE}& \multicolumn{2}{c}{95\% CI}  &  $h_{CCT}$   &   $b_{CCT}$ & $h_{IK}$  \\ 
		& Original & Corrected &&&&& \\ \hline
		& \multicolumn{7}{c}{Panel A: Average verbal score} \\ \cline{2-8}
		Wild bootstrap    & -0.449 & -0.575 & (-1.100        & 0.131 )      & 12.391  &  18.278 &  \\
		CCT robust        & -0.449 & -0.575 & (-1.111        & -0.040)      & 12.391  &  18.278 &  \\
		Conventional      & -0.488 &        & (-1.104        & 0.129 )      &         &         & 7.952 \\
		
		&\multicolumn{7}{c}{Panel B: Average math score} \\ \cline{2-8}
		Wild bootstrap    & -0.185 & -0.263 & (-0.924        & 0.466)       & 11.612  &  17.683 &  \\
		CCT robust        & -0.185 & -0.272 & (-0.884        & 0.340)       & 11.612  &  17.683 &  \\
		Conventional      & -0.202 &        & (-0.802        & 0.398)       &         &         & 9.200 \\ \hline 
	\end{tabular}
	\caption{The effect of class size on average verbal score and average math score.}
	\label{tb: the effect of class size}
\end{table}

\section{Conclusion}

A new wild bootstrap procedure is proposed to correct bias and construct valid confidence interval in fuzzy RD designs. This new method builds upon the developments and intuition advanced by CCT but is implemented through a novel iterated bootstrap. In particular, the local second order models are estimated for generating bootstrap samples. The first layer of bootstrap is performed in order to obtain the empirical distribution of bias-corrected treatment effect, which is made possible by utilizing a second layer of bootstrap to estimate the bias from linear models. This new procedure is proved to be theoretically valid and empirically supported by simulation studies.



\appendix
\section{Appendix}
This appendix adopts CCT's notation where possible and utilizes some conclusions from that paper. Let $e_p$ be the selection vector with 1 in element $p+1$ and 0 everywhere else and assume, with some abuse of notation, that the dimension of $e_p$ adapts to make matrix and vector operations conformable. Much of the theory in this appendix applies to both sides of the cutoff symmetrically, so I use ``$\bullet$'' as a placeholder for either $+$ or $-$ in equations. Further let $r_p(x) = (1,\ x,\dots,\ x^p)'$, $\1_+(x) = \1\{x \geq 0\}$, $\1_-(x) = \1\{x < 0\}$, $m = \min(h, b)$ and $\nu \le p < q$. Define the following terms related to local polynomial regression:
\begin{align*}
\Gamma_{\bullet,p}(h)
&= \tfrac{1}{n} \sum_{i=1}^n r_p(X_i/h) r_p(X_i/h)' K_{\bullet,h}(X_i) \\
\Gamma_{\bullet,q}(b)
&= \tfrac{1}{n} \sum_{i=1}^n r_q(X_i/b) r_q(X_i/b)' K_{\bullet,b}(X_i) \\
\Bf_{\bullet, \nu, p, q}(h)
& = \nu ! e'_{\nu} \big(\Gamma_{\bullet,p}(h)\big)^{-1}
\tfrac{1}{n} \sum_{i=1}^n (X_i / h)^q r_p(X_i/h) K_{\bullet,h}(X_i).
\end{align*}
When $nh \to \infty$, $nm \to \infty$ and $h \to 0$, CCT's Lemma SA.1 and SA.2 imply that these terms have well-defined limits under Assumptions~\ref{as: DGP behavior} and~\ref{as: bandwidth and kernel}.

Let $\hat\beta_{Z \bullet,p}(h)$ be the coefficient estimators from the weighted regression of $Z_i$ on $r_p(X_i)$:
\begin{align*}
	\hat\beta_{Z \bullet,p}(h) &= H_p(h) \Gamma_{\bullet,p}(h)^{-1}
	\tfrac{1}{n} \sum_{i=1}^n r_p(X_i/h) Z_i K_{\bullet,h}(X_i)
\end{align*}
with $H_p(h) = diag(1, h^{-1}, \dots, h^{-p})$. These coefficients are related to the quantities of interest by
\begin{equation*}
	\hat{\mu}_{Z \bullet,p}^{(\nu)}(h) = \nu! e_\nu' \hat\beta_{Z \bullet,p}(h)
\end{equation*}
and
\begin{equation*}
	\hat \zeta_{\nu, p}(h) =
	\frac{\hat{\mu}_{Y +,p}^{(\nu)}(h) - \hat{\mu}_{Y -,p}^{(\nu)}(h)}
	{\hat{\mu}_{T +,p}^{(\nu)}(h) - \hat{\mu}_{T -,p}^{(\nu)}(h)}
\end{equation*}
for $\nu = 0,\dots,p$.

\subsection{Proof of Theorem \ref{th: bootstrap bias correction}}
\label{se: proof 1}

Based on the bias calculated from Algorithm \ref{al: bias estimation}, the difference between the bias-corrected estimator and the true treatment effect is
\begin{equation*}
	\hat\zeta_{\nu, p}(h) - \Delta_{\nu, p, q}^*(h,b) - \zeta_{\nu} \\
	= (\hat\zeta_{\nu, p}(h) - \zeta) - (\E^* \frac{\hat \tau_{Y, \nu, p}^*(h)}{\hat \tau_{T, \nu, p}^*(h)} - \frac{\tau_{Y, \nu}^*}{\tau_{T, \nu}^*}).
\end{equation*}
The first two terms on the right side can be written as
\begin{align*}
	\hat\zeta_{\nu, p}(h) - \zeta_{\nu}
	= & \frac{1}{\tau_{T, \nu}} (\hat\tau_{Y, \nu, p}(h) - \tau_{Y, \nu}) 
	    - \frac{\tau_{Y, \nu}}{\tau_{T, \nu}^2} (\hat\tau_{T, \nu, p}(h) - \tau_{T, \nu}) \\
	  & + \frac{\tau_{Y, \nu}}{\tau_{T, \nu}^2 \hat \tau_{T, \nu, p}} (\hat\tau_{T, \nu, p}(h) - \tau_{T, \nu})^2 
	    - \frac{1}{\tau_{T, \nu} \hat \tau_{T, \nu, p}} (\hat\tau_{Y, \nu, p}(h) - \tau_{Y, \nu}) (\hat\tau_{T, \nu, p}(h) - \tau_{T, \nu}) \\
	= & \frac{1}{\tau_{T, \nu}} (\hat\tau_{Y, \nu, p}(h) - \tau_{Y, \nu}) 
	    - \frac{\tau_{Y, \nu}}{\tau_{T, \nu}^2} (\hat\tau_{T, \nu, p}(h) - \tau_{T, \nu}) + R_n,
\end{align*}
with $R_n = O_p(\frac{1}{nh^{1 + 2 \nu}} + h^{2 (p + 1 - \nu)})$ (CCT's Lemma A.2). Similarly, the last two terms on the right side can be written as
\begin{equation*}
	\E^* \frac{\hat \tau_{Y, \nu, p}^*(h)}{\hat \tau_{T, \nu, p}^*(h)} - \frac{\tau_{Y, \nu}^*}{\tau_{T, \nu}^*}
	=  \frac{1}{\tau_{T, \nu}^*} (\E^* \hat \tau_{Y, \nu, p}^*(h) - \tau_{Y, \nu}^*) - 
	   \frac{\tau_{Y, \nu}^*}{\tau_{T, \nu}^{*2}} (\E^* \hat \tau_{T, \nu, p}^*(h) - \tau_{T, \nu}^*) + R_n^*,
\end{equation*}
with $R_n^* = O_p(\frac{1}{nh^{1 + 2 \nu}} + h^{2 (p + 1 - \nu)})$. By construction of the wild bootstrap DGP,
\begin{equation*}
	Z_i^* =
	\begin{cases}
	r_q(X_i/b)'H_q(b)^{-1} \beta_{Z+,q}^* + \varepsilon_i^* & X_i \geq 0 \\
	r_q(X_i/b)'H_q(b)^{-1} \beta_{Z-,q}^* + \varepsilon_i^* & X_i < 0,
	\end{cases}
\end{equation*}
with $\beta_{Z+,q}^*$ and $\beta_{Z-,q}^*$ being the true parameters in the bootstrap data. Equivalently, $\mu_{Z \bullet}^{*(\nu)} = \nu! e_\nu' \beta_{Z \bullet,q}^*$ is the true treatment effect in the bootstrap data. CCT's Lemma SA.3 indicates that
\begin{align*}
	\E^* \hat\mu_{Z \bullet, p}^{* (\nu)}(h) - \mu_{Z \bullet}^{* (\nu)}
	= h^{1 + p - \nu} \mu_{Z \bullet}^{*(1 + p)} \Bf_{\bullet, \nu, p, 1+p}(h)/(1 + p)! + O_p(h^{2 + p - \nu}),
\end{align*}
which allows for an analytical form of the bias in the bootstrap data:
\begin{equation*}
	\E^* \hat \tau_{Z, \nu, p}^*(h) - \tau_{Z, \nu}^* 
	= h^{1 + p - \nu} \big(\mu_{Z+}^{*(1 + p)} \Bf_{+, \nu, p, p+1}(h) - \mu_{Z-}^{*(1 + p)} \Bf_{-, \nu, p, p+1}(h) \big)/(1 + p)! + O_p(h^{2 + p - \nu}).
\end{equation*}
Notice that CCT's bias term is only slightly different from this. They use the following formula for bias correction:
\begin{equation*}
	\hat\tau_{Z, \nu, p, q}^{bc} (h, b) = 
	\hat\tau_{Z, \nu, p} (h) - 
	h^{1 + p - \nu} \big(\hat \mu_{Z+, q}^{(1 + p)} \Bf_{+, \nu, p, p+1}(h) - \hat \mu_{Z-, q}^{(1 + p)} \Bf_{-, \nu, p, p+1}(h) \big)/(1 + p)!.
\end{equation*}
Built on above preparations, it can be shown that
\begin{equation}
	\label{eq: normality of bootstrap bias-corrected estimator}
	\hat\zeta_{\nu, p}(h) - \Delta_{\nu, p, q}^*(h,b) - \zeta_{\nu} 
	=  \frac{1}{\tau_{T, \nu}} (\hat\tau_{Y, \nu, p, q}^{bc} (h, b) - \tau_{Y, \nu}) - 
	   \frac{\tau_{Y, \nu}}{\tau_{T, \nu}^2} (\hat\tau_{T, \nu, p, q}^{bc} (h, b) - \tau_{T, \nu}) +  
	   R_n - R_n^* - R_n^{*bc} + O_p(h^{2 + p - \nu}),
\end{equation}
where $R_n^{*bc}$ is defined by:
\begin{align*}
R_n^{*bc}
	= & \frac{1}{\tau_{T, \nu}^*} h^{1 + p - \nu} 
	    \big(\mu_{Y+}^{*(1 + p)} \Bf_{+, \nu, p, p+1}(h) - \mu_{Y-}^{*(1 + p)} \Bf_{-, \nu, p, p+1}(h) \big)/(1 + p)! \\
	& - \frac{\tau_{Y, \nu}^*}{\tau_{T. \nu}^{*2}} h^{1 + p - \nu} 
	    \big(\mu_{T+}^{*(1 + p)} \Bf_{+, \nu, p, p+1}(h) - \mu_{T-}^{*(1 + p)} \Bf_{-, \nu, p, p+1}(h) \big)/(1 + p)! \\
	& - \frac{1}{\tau_{T,\nu}} h^{1 + p - \nu} 
	    \big(\hat \mu_{Y+, q}^{(1 + p)} \Bf_{+, \nu, p, p+1}(h) - \hat \mu_{Y-, q}^{(1 + p)} \Bf_{-, \nu, p, p+1}(h) \big)/(1 + p)! \\
	& + \frac{\tau_{Y,\nu}}{\tau_{T,\nu}^2} h^{1 + p - \nu} 
	    \big(\hat \mu_{T+, q}^{(1 + p)} \Bf_{+, \nu, p, p+1}(h) - \hat \mu_{T-, q}^{(1 + p)} \Bf_{-, \nu, p, p+1}(h) \big)/(1 + p)! \\
	= & \frac{1}{\hat\tau_{T, \nu, q}(b)} h^{1 + p - \nu} 
	    \big(\hat \mu_{Y+, q}^{(1 + p)} \Bf_{+, \nu, p, p+1}(h) - \hat \mu_{Y-, q}^{(1 + p)} \Bf_{-, \nu, p, p+1}(h) \big)/(1 + p)! \\
	& - \frac{\hat\tau_{Y, \nu, q}(b)}{\hat\tau_{T, \nu, q}^{2}(b)} h^{1 + p - \nu} 
	    \big(\hat \mu_{T+, q}^{(1 + p)} \Bf_{+, \nu, p, p+1}(h) - \hat \mu_{T-, q}^{(1 + p)} \Bf_{-, \nu, p, p+1}(h) \big)/(1 + p)! \\
	& - \frac{1}{\tau_{T,\nu}} h^{1 + p - \nu} 
		\big(\hat \mu_{Y+, q}^{(1 + p)} \Bf_{+, \nu, p, p+1}(h) - \hat \mu_{Y-, q}^{(1 + p)} \Bf_{-, \nu, p, p+1}(h) \big)/(1 + p)! \\
	& + \frac{\tau_{Y,\nu}}{\tau_{T,\nu}^2} h^{1 + p - \nu} 
		\big(\hat \mu_{T+, q}^{(1 + p)} \Bf_{+, \nu, p, p+1}(h) - \hat \mu_{T-, q}^{(1 + p)} \Bf_{-, \nu, p, p+1}(h) \big)/(1 + p)! \\
	= & (\frac{1}{\hat\tau_{T, \nu, q}(b)} - \frac{1}{\tau_{T,\nu}}) h^{1 + p - \nu} 
	    \big(\hat \mu_{Y+, q}^{(1 + p)} \Bf_{+, \nu, p, p+1}(h) - \hat \mu_{Y-, q}^{(1 + p)} \Bf_{-, \nu, p, p+1}(h) \big)/(1 + p)! \\
	& - (\frac{\hat\tau_{Y, \nu, q}(b)}{\hat\tau_{T, \nu, q}^{2}(b)} - \frac{\tau_{Y,\nu}}{\tau_{T,\nu}^2}) h^{1 + p - \nu} 
	    \big(\hat \mu_{T+, q}^{(1 + p)} \Bf_{+, \nu, p, p+1}(h) - \hat \mu_{T-, q}^{(1 + p)} \Bf_{-, \nu, p, p+1}(h) \big)/(1 + p)! \\
	= & h^{1 + p - \nu} O_p(\frac{1}{\sqrt{nb^{1 + 2 \nu}}} + b^{1 + q - \nu}) O_p(1 + \frac{1}{\sqrt{nb^{3 + 2p}}}).
\end{align*}
The second equality holds because $\mu_{Z \bullet}^{*(1+p)} = \hat\mu_{Z \bullet, q}^{(1+p)}(b)$ and $\tau_{Z, \nu}^* = \hat \tau_{Z, \nu, q} (b)$ almost surely because the bootstrap DGP is obtained by fitting a local polynomials of order $q$. The last equality holds because of similar argument in CCT's Theorem A.2. Asymptotic normality of $\hat\zeta_{\nu, p}(h) - \Delta_{\nu, p, q}^*(h,b) - \zeta_{\nu}$ then follows from normality of $\hat\tau_{Y, \nu, p, q}^{bc} (h, b) - \tau_{Y, \nu}$, $\hat\tau_{T, \nu, p, q}^{bc} (h, b) - \tau_{T, \nu}$ (CCT's Theorem 1) and the fact that remaining terms $R_n$, $R_n^*$, $R_n^{*bc}$ and $O_p(h^{2 + p - \nu})$ are negligible. 

CCT have shown that $V^{bc}(h,b) = O_p(\frac{1}{nh^{1 + 2 \nu}} + \frac{h^{2 (1 + p - \nu)}}{nb^{3 + 2p}})$ (Lemma SA.4) and $R_n^2 = o_p(V^{bc}(h,b))$ (Theorem A.2). In addition, because $O_p(h^{2 + p - \nu}) = o_p(R_n^{*bc})$, it suffices to show that
\begin{align*}
	\frac{{R_n^{*bc}}^2}{V^{bc}(h,b)}
	= & O_p \big(\min\{nh^{1 + \nu}, \frac{nb^{3 + 2p}}{h^{2(1 + p - \nu)}}\} \big) h^{2(1 + p - \nu)}
	    O_p \big(\frac{1}{nb^{1 + 2 \nu}} + b^{2(1 + q - \nu)} \big) 
	    O_p \big(1 + \frac{1}{nb^{3 + 2p}} \big) \\
	= & O_p \big(\min\{nh^{3 + 2p}, nb^{3 + 2p}\} \big)
		O_p \big(\frac{1}{nb^{1 + 2 \nu}} + b^{2(1 + q - \nu)} \big) 
		O_p \big(1 + \frac{1}{nb^{3 + 2p}} \big) \\
	= & O_p \big(b^{2 + 2(p - \nu)} \min\{ (\frac{h}{b})^{3 + 2p}, 1\} 
	             + nb^{2(1 + q - \nu)} \min\{nh^{3 + 2p}, nb^{3 + 2p}\} \big)
	    O_p \big(1 + \frac{1}{nb^{3 + 2p}} \big) \\
	= & O_p \big(b^{2 + 2(p - \nu)} \min\{ (\frac{h}{b})^{3 + 2p}, 1\} 
	             + nb^{2(q - p)}b^{2(1 + p - \nu)} \min\{nh^{3 + 2p}, nb^{3 + 2p}\} \big) \\
	  & + O_p \big(\frac{1}{nb^{1 + 2\nu}} \min\{ (\frac{h}{b})^{3 + 2p}, 1\} 
	             + b^{2(1 + q - \nu)} \min\{ (\frac{h}{b})^{3 + 2p}, 1\} \big) \\
	= & o_p(1),
\end{align*}
provided that $n \min \{h^{3 + 2p}, b^{3 + 2p} \} \max \{h^2, b^{2(q - p)}\} \to 0$ and $n \min \{h, b^{1 + 2 \nu}\} \to \infty$.
\qed.


\subsection{Proof of Theorem \ref{th: bootstrap confidence intervals}}
\label{se: proof 2}

Repeat the steps from Theorem \ref{th: bootstrap bias correction}'s proof for the iterated bootstrap to get
\begin{equation*}
	\hat\zeta_{\nu, p}^*(h) - \Delta_{\nu, p, q}^{**}(h,b) - \zeta_{\nu}^* 
	=  \frac{1}{\tau_{T, \nu}^*} (\hat\tau_{Y, \nu, p, q}^{*bc} (h, b) - \tau_{Y, \nu}^*) - 
	\frac{\tau_{Y, \nu}^*}{\tau_{T, \nu}^{*2}} (\hat\tau_{T, \nu, p, q}^{*bc} (h, b) - \tau_{T, \nu}^*) +  
	R_n^* - R_n^{**} - R_n^{**bc} + O_p(h^{2 + p - \nu}),
\end{equation*}
As is proved in previous section, the higher order terms do not contribute to its asymptotic variance and can be ignored. It will be firstly shown that the variance of $\frac{1}{\tau_{T, \nu}^*} (\hat\tau_{Y, \nu, p, q}^{*bc} (h, b) - \tau_{Y, \nu}^*) - \frac{\tau_{Y, \nu}^*}{\tau_{T, \nu}^{*2}} (\hat\tau_{T, \nu, p, q}^{*bc} (h, b) - \tau_{T, \nu}^*)$ converges to that of $\frac{1}{\tau_{T, \nu}} (\hat\tau_{Y, \nu, p, q}^{bc} (h, b) - \tau_{Y, \nu}) - \frac{\tau_{Y, \nu}}{\tau_{T, \nu}^2} (\hat\tau_{T, \nu, p, q}^{bc} (h, b) - \tau_{T, \nu})$, then its asymptotic normality will be proved.

\textbf{Proof for variance convergence in probability.} Rewrite bias-corrected estimator for $Z$:
\begin{align*}
	\hat\tau_{Z, \nu, p, q}^{bc} (h, b) - \tau_{Z, \nu}
	=& (\hat\tau_{Z, \nu, p}(h) - \E \hat \tau_{Z, \nu, p}(h)) +
	(\E \hat \tau_{Z, \nu, p}(h) - \tau_{Z, \nu}) - (\E^{*} \hat \tau_{Z, \nu, p}^{*}(h) - \tau_{Z, \nu}^{*}) \\
	=& \hat\tau_{Z, \nu, p}(h) - \E \hat \tau_{Z, \nu, p}(h) \\
	 & + h^{1 + p - \nu} (\hat\mu_{Z-,q}^{(q)}(b) - \mu_{Z-}^{(q)}) \Bf_{-, \nu, p, p + 1}(h) /(1 + p)! \\
	 & - h^{1 + p - \nu} (\hat\mu_{Z+,q}^{(q)}(b) - \mu_{Z+}^{(q)}) \Bf_{+, \nu, p, p + 1}(h) /(1 + p)! \\
	 & + O_p(h^{2 + p - \nu}) \\
	=& \nu! e_\nu' \Gamma_{+,p}(h)^{-1} \Big(\tfrac{1}{n} \sum_{i=1}^n r_p(X_i/h) K_{+,h}(X_i) \varepsilon_{Zi}\Big) \\
	 & - \nu! e_\nu' \Gamma_{-,p}(h)^{-1} \Big(\tfrac{1}{n} \sum_{i=1}^n r_p(X_i/h) K_{-,h}(X_i) \varepsilon_{Zi}\Big) \\
	 & + \frac{q! e_q' h^{1 + p - \nu}}{(1 + p)! b^q} \Gamma_{-,q}(b)^{-1}
	\Big(\tfrac{1}{n} \sum_{i=1}^n r_q(X_i/b) K_{-,b}(X_i) \varepsilon_{Zi} \Big) \Bf_{-, \nu, p, p + 1}(h) \\
	 & - \frac{q! e_q' h^{1 + p - \nu}}{(1 + p)! b^q} \Gamma_{+,q}(b)^{-1}
	\Big(\tfrac{1}{n} \sum_{i=1}^n r_q(X_i/b) K_{+,b}(X_i) \varepsilon_{Zi} \Big) \Bf_{+, \nu, p, p + 1}(h) \\
	& + O_p(h^{2 + p - \nu}) \\
	=& \sum_{i = 1}^{n} W(X_i) \varepsilon_{Zi} + O_p(h^{2 + p - \nu})
\end{align*}
with
\begin{align*}
	W(X_i) & = W_+(X_i) - W_-(X_i) \\  
	W_\bullet(X_i) & = \tfrac{1}{n} \nu! e_\nu' \Gamma_{\bullet,p}(h)^{-1} r_p(X_i/h) K_{\bullet,h}(X_i) 
	- \tfrac{1}{n} \frac{q! e_q' h^{1 + p - \nu}}{(1 + p)! b^q} \Gamma_{\bullet,q}(b)^{-1} r_q(X_i/b) K_{\bullet,b}(X_i).
\end{align*}
With this simplified notation, we have
\begin{align*}
	\frac{1}{\tau_{T, \nu}} (\hat\tau_{Y, \nu, p, q}^{bc} (h, b) - \tau_{Y, \nu}) - 
	\frac{\tau_{Y, \nu}}{\tau_{T, \nu}^2} (\hat\tau_{T, \nu, p, q}^{bc} (h, b) - \tau_{T, \nu})
	& = \sum_{i = 1}^{n} W(X_i) (\frac{1}{\tau_{T, \nu}} \varepsilon_{Yi} - \frac{\tau_{Y, \nu}}{\tau_{T, \nu}^2} \varepsilon_{Ti}) + O_p(h^{2 + p - \nu}),
\end{align*}
which has variance
\begin{equation*}
	\V \big(\sum_{i = 1}^{n} W(X_i) (\frac{1}{\tau_{T, \nu}} \varepsilon_{Yi} - \frac{\tau_{Y, \nu}}{\tau_{T, \nu}^2} \varepsilon_{Ti})\big)
	= \sum_{i = 1}^{n} W(X_i)^2 (\frac{1}{\tau_{T, \nu}^2} \sigma_{Yi}^2 + \frac{\tau_{Y, \nu}^2}{\tau_{T, \nu}^4} \sigma_{Ti}^2 - 
	  \frac{2 \tau_{Y, \nu}}{\tau_{T, \nu}^3} \sigma_{Yi, Ti}).
\end{equation*}
Apply similar steps to the iterated bootstrap, we have
\begin{equation*}
	\frac{1}{\tau_{T, \nu}^*} (\hat\tau_{Y, \nu, p, q}^{*bc} (h, b) - \tau_{Y, \nu}^*) - 
	\frac{\tau_{Y, \nu}^*}{\tau_{T, \nu}^{*2}} (\hat\tau_{T, \nu, p, q}^{*bc} (h, b) - \tau_{T, \nu}^*)
	= \sum_{i = 1}^{n} W(X_i) (\frac{1}{\tau_{T, \nu}^*} \varepsilon_{Yi}^* - \frac{\tau_{Y, \nu}^*}{\tau_{T, \nu}^{*2}} \varepsilon_{Ti}^*), 
\end{equation*}
which, by the construction of wild bootstrap, has variance
\begin{equation*}
	\V^* \big(\sum_{i = 1}^{n} W(X_i) (\frac{1}{\tau_{T, \nu}^*} \varepsilon_{Yi}^* - \frac{\tau_{Y, \nu}^*}{\tau_{T, \nu}^{*2}} \varepsilon_{Ti}^*)\big)
	= \sum_{i = 1}^{n} W(X_i)^2 (\frac{1}{\tau_{T, \nu}^{*2}} \hat \varepsilon_{Yi}^2 + \frac{\tau_{Y, \nu}^{*2}}{\tau_{T, \nu}^{*4}} \hat\varepsilon_{Ti}^2 - \frac{2 \tau_{Y, \nu}^*}{\tau_{T, \nu}^{*3}} \hat \varepsilon_{Yi} \hat \varepsilon_{Ti}).
\end{equation*}
By the standard argument on the convergence of residuals to the population error, it is ensured that $\sum_{i = 1}^{n} W(X_i)^2 \hat \varepsilon_{Yi}^2 \to^p \sum_{i = 1}^{n} W(X_i)^2 \sigma_{Yi}^2$, $\sum_{i = 1}^{n} W(X_i)^2 \hat \varepsilon_{Ti}^2 \to^p \sum_{i = 1}^{n} W(X_i)^2 \sigma_{Ti}^2$ and $\sum_{i = 1}^{n} W(X_i)^2 \hat \varepsilon_{Yi} \hat \varepsilon_{Ti} \to^p \sum_{i = 1}^{n} W(X_i)^2 \sigma_{Yi, T_i}$. Combined with the fact that $\tau_{Z, \nu}^* = \hat \tau_{Z,q} (b) \to^p \tau_Z$, the proof for convergence of variance is complete.

\textbf{Proof for asymptotic normality.} Conditional on the regressors and residuals, $\{W(X_i) (\frac{1}{\tau_T^*} \hat \varepsilon_{Yi} - \frac{\tau_Y^*}{\tau_T^{*2}} \hat \varepsilon_{Ti}) e_i^*\}$ is a sequence of independent and mean zero random variables. In addition, it consists of four parts based on the definition of $W(X_i)$. It can be shown that each part is asymptotically normal by Lindeberg-Feller CLT. The proof below is an example showing that the first part $\tfrac{1}{n} \nu! e_\nu' \Gamma_{\bullet,p}(h)^{-1} r_p(X_i/h) K_{\bullet,h}(X_i) (\frac{1}{\tau_T^*} \hat \varepsilon_{Yi} - \frac{\tau_Y^*}{\tau_T^{*2}} \hat \varepsilon_{Ti}) e_i^*$ is asymptotically normal.

The Liapunov's condition requires that
\begin{equation*}
\frac{1}{s_n^{2 + \delta}} \sum_{i=1}^{n} \E \lvert H_i(Xi) \rvert^{2 + \delta} \to^p 0
\end{equation*}
with
\begin{equation*}
H_i(X_i) = \tfrac{1}{n} \nu! e_\nu' \Gamma_{\bullet,p}(h)^{-1} r_p(X_i/h) K_{\bullet,h}(X_i) (\frac{1}{\tau_T^*} \hat \varepsilon_{Yi} - \frac{\tau_Y^*}{\tau_T^{*2}} \hat \varepsilon_{Ti}) e_i^*; \quad
s_n^2 = \sum_{i=1}^{n} \V (H_i).
\end{equation*}
Based on CCT's Lemma SA.1, we know that
\begin{align*}
\sum_{i=1}^{n} \E \lvert H_i(Xi) \rvert^{2 + \delta} & = O_p(\frac{1}{(nh)^{1 + \delta}}),  \\
s_n^2 & = O_p(\frac{1}{nh}),
\end{align*}
which verifies the Liapunov's condition given that $nh \to \infty$. Similar arguments can be applied to other three parts.
\qed.




\bibliographystyle{jpe}
\bibliography{reference}
\end{document}

